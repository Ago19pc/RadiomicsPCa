{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e41a722",
   "metadata": {},
   "source": [
    "# Polyjuice Potion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438e84c",
   "metadata": {},
   "source": [
    "You might be asking what is this...\n",
    "\n",
    "Just a mix and mash of ADC, T2W and so on\n",
    "\n",
    "TODO: Definire classe 1 e 2 per esempio classe 1: ISUP 2,3 mentre classe 2: ISUP 3,4.\n",
    "\n",
    "Una classe puo avere 1 o piu valori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06463c43",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9c41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7227f",
   "metadata": {},
   "source": [
    "# Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a115ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_string(param):\n",
    "    return isinstance(param, str)\n",
    "\n",
    "def is_list_of_strings(param):\n",
    "    return isinstance(param, (list, tuple)) and all(isinstance(item, str) for item in param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a610426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_polyjuice_getter(sequence, dataset):\n",
    "    # Filter the dataset to include only AI annotations and selected sequences and ROI lesion\n",
    "    filtered_dataset = dataset[dataset['annotator'] == 'AI']\n",
    "    filtered_dataset = filtered_dataset[filtered_dataset['sequence'] == sequence]\n",
    "    filtered_dataset = filtered_dataset[filtered_dataset['ROI'] == 'lesion']\n",
    "    filtered_dataset.drop(columns=['annotator', 'sequence', 'ROI_ID', 'ROI','img_path', 'seg_path', 'extraction_ID'], inplace=True)\n",
    "    dataset = filtered_dataset\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700a59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyjuice_getter(sequence, dataset):\n",
    "    polyjuice_dataset = None\n",
    "    filtered_dataset = dataset[dataset['annotator'] == 'AI']\n",
    "    filtered_dataset = filtered_dataset[filtered_dataset['ROI'] == 'lesion']\n",
    "    filtered_dataset.drop(columns=['annotator', 'ROI_ID', 'ROI','img_path', 'seg_path', 'extraction_ID'], inplace=True)\n",
    "\n",
    "    for index, s in enumerate(sequence):\n",
    "        s_dataset = filtered_dataset[filtered_dataset['sequence'] == s]\n",
    "        s_dataset = s_dataset.drop(columns=['sequence'])\n",
    "        for col in s_dataset.columns:\n",
    "            if col != 'patient_ID' and col != 'study_ID':\n",
    "                s_dataset = s_dataset.rename(columns={col : str(col + '_' + s)})\n",
    "        if index == 0:\n",
    "            polyjuice_dataset = s_dataset\n",
    "        else:\n",
    "            polyjuice_dataset = pd.merge(polyjuice_dataset, s_dataset, on=['patient_ID', 'study_ID'], how='inner')\n",
    "\n",
    "    return polyjuice_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a0638c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleSequence = Literal['t2w', 'adc', 'hbv']\n",
    "\n",
    "\n",
    "def get_data(sequence : SingleSequence | str | list[str], isup_class1 : list[int] = [0, 1, 2], isup_class2 : list[int] = [3, 4, 5], corr_cutoff=0.9):\n",
    "    '''\n",
    "    @param squence : What sequence do you want to study. A list of multiple sequence will also work!\n",
    "    @param isup_class1 : What ISUP values do you want in the first class\n",
    "    @param isup_class2 : What ISUP values do you want in the second class\n",
    "    @param corr_cutoff : The value to cutoff high correlated features\n",
    "\n",
    "    @return pd.DataFrame : a new columns 'is_class1' is added, specofing if the row is in class1\n",
    "\n",
    "    Please read the code and the comments, especially the one at the bottom! Some unwanted columns may still be present\n",
    "    '''\n",
    "\n",
    "    is_polyjuice = None\n",
    "    if is_list_of_strings(sequence):\n",
    "        # Is a vector and not a string\n",
    "        if len(sequence) > 1:\n",
    "            # Len > 1 so we are in a polyjuice (mix)\n",
    "            is_polyjuice = True\n",
    "        else:\n",
    "            is_polyjuice = False\n",
    "    else:\n",
    "        is_polyjuice = False\n",
    "\n",
    "    labels = pd.read_csv('marksheet.csv')\n",
    "    dataset = pd.read_csv('PI-CAI_features')\n",
    "\n",
    "    new_dataset = None\n",
    "    if is_polyjuice == False:\n",
    "        new_dataset = no_polyjuice_getter(sequence, dataset)\n",
    "    else:\n",
    "        new_dataset = polyjuice_getter(sequence, dataset)\n",
    "\n",
    "\n",
    "    # The commented code below was an assumption requested in earlier version of this code, just uncomment if needed\n",
    "    # We only want Magnetic Resonace Biopsy (MRBx) labels because Systematic Biopsy (SBx) labels are not for our usecase\n",
    "    # we also remove those that have both because clicinians result might be biased\n",
    "    #labels = labels[labels['histopath_type'] == 'MRBx']\n",
    "\n",
    "    \n",
    "    labels.rename(columns={'patient_id': 'patient_ID', 'study_id': 'study_ID'}, inplace=True)\n",
    "    labels.drop(columns=['mri_date', 'histopath_type', 'center', 'lesion_ISUP', 'lesion_GS'], inplace=True)\n",
    "    \n",
    "\n",
    "    # Remove high correlated features\n",
    "    corr_matrix = new_dataset.drop(columns=['study_ID', 'patient_ID']).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > corr_cutoff)]\n",
    "    new_dataset.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Drop missing values, perhaps a better way might be useful. What about KNNImputer? If so, maybe checking that we don't miss much values before imputing new ones\n",
    "    #labels.dropna(inplace=True)\n",
    "    \n",
    "    labels.drop(columns=['case_csPCa'], inplace=True)                           # We drop it because after merge all remaining patients have cancer\n",
    "\n",
    "    merge = pd.merge(new_dataset, labels, on=['patient_ID', 'study_ID'], how='inner')\n",
    "    merge.drop(columns=['patient_ID', 'study_ID'], inplace=True)\n",
    "\n",
    "\n",
    "    isup = 'case_ISUP'\n",
    "\n",
    "\n",
    "    if sum(isup_class1 + isup_class2) == 15 and len(isup_class1 + isup_class2) == 6:\n",
    "        # class1 and class2 contains all possible values (from 0 to 5)\n",
    "        merge['is_class1'] = merge[isup].map(lambda x: 1 if x in isup_class1 else 0)\n",
    "    else:\n",
    "        # class1 and class2 don't contain all possible values, so we need to drop some\n",
    "        merge = merge[merge[isup].isin(isup_class1 + isup_class2)]\n",
    "        merge['is_class1'] = merge[isup].map(lambda x: 1 if x in isup_class1 else 0)\n",
    "\n",
    "    merge.drop(columns=[isup], inplace=True)    # We drop it because it been encoded in is_class1\n",
    "\n",
    "\n",
    "    return merge\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134b475",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TypeScaler = Literal['standard', 'robust']\n",
    "\n",
    "def scale(data : pd.DataFrame, scaler : str = 'standard', target='is_class1') -> pd.DataFrame:\n",
    "\n",
    "    num_feat = list(set(data.columns) - set(target))\n",
    "    to_ret = data.copy()\n",
    "    if scaler == 'standard':\n",
    "        s = StandardScaler()\n",
    "        to_ret[num_feat] = s.fit_transform(data[num_feat])\n",
    "        return to_ret\n",
    "    elif scaler == 'Robust':\n",
    "        s = RobustScaler()\n",
    "        to_ret[num_feat] = s.fit_transform(data[num_feat])\n",
    "        return to_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1c69e",
   "metadata": {},
   "source": [
    "# Automatization, first steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def do_models(models : dict, data : pd.DataFrame, draw : bool = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['is_class1']), data['is_class1'], test_size=0.2)\n",
    "    \n",
    "    model_auc = []\n",
    "\n",
    "    for name, model in zip(models.keys(), models.values()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "        \n",
    "        if draw:\n",
    "            print(name)\n",
    "            print(report)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Curva ROC')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        model_auc.append((name, auc))\n",
    "    \n",
    "    return model_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagle(sequence, corr_cutoff, isup_class0, isup_class1, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1198c",
   "metadata": {},
   "source": [
    "# ok do your stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437153b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('t2w',), [('MLP', np.float64(0.592760180995475)), ('Random Forest', np.float64(0.5294117647058824))]), (('adc',), [('MLP', np.float64(0.5800438596491228)), ('Random Forest', np.float64(0.5581140350877194))]), (('hbv',), [('MLP', np.float64(0.5788177339901478)), ('Random Forest', np.float64(0.6428571428571428))]), (('t2w', 'adc'), [('MLP', np.float64(0.5441176470588235)), ('Random Forest', np.float64(0.5690045248868778))]), (('t2w', 'hbv'), [('MLP', np.float64(0.5307692307692308)), ('Random Forest', np.float64(0.5987179487179487))]), (('adc', 'hbv'), [('MLP', np.float64(0.5474358974358974)), ('Random Forest', np.float64(0.5602564102564103))]), (('t2w', 'adc', 'hbv'), [('MLP', np.float64(0.48263888888888884)), ('Random Forest', np.float64(0.59375))])]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(3):\n",
    "    for combo in combinations(['t2w', 'adc', 'hbv'], r=i+1):\n",
    "\n",
    "        cleaned_data = clean_data(data=get_data(['t2w', 'adc', 'hbv'], isup_class1=[2], isup_class2=[3]))\n",
    "        models = {\n",
    "            'MLP' : MLPClassifier(max_iter=1000, random_state=42, hidden_layer_sizes=(100, 50, 50, 50, 100, 50), activation='relu', solver='adam'),\n",
    "            'Random Forest': RandomForestClassifier()\n",
    "        }\n",
    "\n",
    "        results.append((combo, do_models(models, cleaned_data, draw=False)))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cafd01e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2w\n",
      "MLP 0.592760180995475\n",
      "Random Forest 0.5294117647058824\n",
      "adc\n",
      "MLP 0.5800438596491228\n",
      "Random Forest 0.5581140350877194\n",
      "hbv\n",
      "MLP 0.5788177339901478\n",
      "Random Forest 0.6428571428571428\n",
      "t2w\n",
      "MLP 0.5441176470588235\n",
      "Random Forest 0.5690045248868778\n",
      "t2w\n",
      "MLP 0.5307692307692308\n",
      "Random Forest 0.5987179487179487\n",
      "adc\n",
      "MLP 0.5474358974358974\n",
      "Random Forest 0.5602564102564103\n",
      "t2w\n",
      "MLP 0.48263888888888884\n",
      "Random Forest 0.59375\n"
     ]
    }
   ],
   "source": [
    "for combo, mod_val in results:\n",
    "    print(combo[0])\n",
    "    for mod, val in mod_val:\n",
    "        print(mod, val.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4db4107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_data(sequence=['t2w', 'adc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3a528bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 616)\n",
      "(391, 615)\n"
     ]
    }
   ],
   "source": [
    "print(d.shape)\n",
    "print(d.drop(columns=['psad']).dropna().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
